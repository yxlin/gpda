% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Apgar2010,
  Title                    = {Sloppy models, parameter uncertainty, and the role of experimental design},
  Author                   = {Apgar, Joshua F and Witmer, David K and White, Forest M and Tidor, Bruce},
  Journal                  = {Molecular BioSystems},
  Year                     = {2010},
  Number                   = {10},
  Pages                    = {1890--1900},
  Volume                   = {6},

  Doi                      = {10.1039/B918098B},
  Owner                    = {yslin},
  Timestamp                = {2017.05.17}
}

@Article{Ball1982,
  Title                    = {A specific and enduring improvement in visual motion discrimination},
  Author                   = {Ball, K and Sekuler, R},
  Journal                  = {Science},
  Year                     = {1982},
  Number                   = {4573},
  Pages                    = {697--698},
  Volume                   = {218},

  Abstract                 = {Training improves the ability of human observers to discriminate between two similar directions of motion. This gradual improvement is specific to the direction on which an observer is trained, and it endures for several months. Improvement does not affect motion perception generally, nor does it depend on recognition of details of the movement.},
  Doi                      = {10.1126/science.7134968},
  ISSN                     = {0036-8075},
  Owner                    = {yslin},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2017.05.24}
}

@Article{Beaumont2010,
  Title                    = {Approximate {Bayesian} {Computation} in {Evolution} and {Ecology}},
  Author                   = {Beaumont, Mark A.},
  Journal                  = {Annual Review of Ecology, Evolution, and Systematics},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {379--406},
  Volume                   = {41},

  Abstract                 = {In the past 10years a statistical technique, approximate Bayesian computation (ABC), has been developed that can be used to infer parameters and choose between models in the complicated scenarios that are often considered in the environmental sciences. For example, based on gene sequence and microsatellite data, the method has been used to choose between competing models of human demographic history as well as to infer growth rates, times of divergence, and other parameters. The method fits naturally in the Bayesian inferential framework, and a brief overview is given of the key concepts. Three main approaches to ABC have been developed, and these are described and compared. Although the method arose in population genetics, ABC is increasingly used in other fields, including epidemiology, systems biology, ecology, and agent-based modeling, and many of these applications are briefly described.},
  Doi                      = {10.1146/annurev-ecolsys-102209-144621},
  File                     = {Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/AIEQE8JG/2010 - Approximate Bayesian Computation in Evolution and .pdf:application/pdf},
  Urldate                  = {2017-03-09}
}

@Article{Braak2006,
  Title                    = {A Markov Chain Monte Carlo version of the genetic algorithm Differential Evolution: easy Bayesian computing for real parameter spaces},
  Author                   = {Braak, Cajo J. F. Ter},
  Journal                  = {Statistics and Computing},
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {239--249},
  Volume                   = {16},

  Abstract                 = {Differential Evolution (DE) is a simple genetic algorithm for numerical optimization in real parameter spaces. In a statistical context one would not just want the optimum but also its uncertainty. The uncertainty distribution can be obtained by a Bayesian analysis (after specifying prior and likelihood) using Markov Chain Monte Carlo (MCMC) simulation. This paper integrates the essential ideas of DE and MCMC, resulting in Differential Evolution Markov Chain (DE-MC). DE-MC is a population MCMC algorithm, in which multiple chains are run in parallel. DE-MC solves an important problem in MCMC, namely that of choosing an appropriate scale and orientation for the jumping distribution. In DE-MC the jumps are simply a fixed multiple of the differences of two random parameter vectors that are currently in the population. The selection process of DE-MC works via the usual Metropolis ratio which defines the probability with which a proposal is accepted. In tests with known uncertainty distributions, the efficiency of DE-MC with respect to random walk Metropolis with optimal multivariate Normal jumps ranged from 68{\%} for small population sizes to 100{\%} for large population sizes and even to 500{\%} for the 97.5{\%} point of a variable from a 50-dimensional Student distribution. Two Bayesian examples illustrate the potential of DE-MC in practice. DE-MC is shown to facilitate multidimensional updates in a multi-chain ``Metropolis-within-Gibbs'' sampling approach. The advantage of DE-MC over conventional MCMC are simplicity, speed of calculation and convergence, even for nearly collinear parameters and multimodal densities.},
  Doi                      = {10.1007/s11222-006-8769-1},
  ISSN                     = {1573-1375},
  Owner                    = {yslin},
  Timestamp                = {2017.03.10}
}

@Article{Brown2008,
  Title                    = {The simplest complete model of choice response time: linear ballistic accumulation},
  Author                   = {Brown, Scott D and Heathcote, Andrew},
  Journal                  = {Cognitive Psychology},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {153--178},
  Volume                   = {57},

  Doi                      = {10.1016/j.cogpsych.2007.12.002},
  File                     = {ScienceDirect Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/PAGCKJUK/Brown and Heathcote - 2008 - The simplest complete model of choice response tim.pdf:application/pdf;ScienceDirect Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/Z3CRZ3DU/S0010028507000722.html:text/html;ScienceDirect Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/AB39NVMW/S0010028507000722.html:text/html},
  Keywords                 = {Choice, Decision, Lexical decision, Mathematical models, Reaction Time, Response time}
}

@Article{Chiu1991,
  Title                    = {Bandwidth selection for kernel density estimation},
  Author                   = {Chiu, Shean-Tsong},
  Journal                  = {The Annals of Statistics},
  Year                     = {1991},
  Number                   = {4},
  Pages                    = {1883--1905},
  Volume                   = {19},

  Abstract                 = {The problem of automatic bandwidth selection for a kernel density estimator is considered. It is well recognized that the bandwidth estimate selected by the least squares cross-validation is subject to large sample variation. This difficulty limits the application of the cross-validation estimate. Based on characteristic functions, an important expression for the cross-validation bandwidth estimate is obtained. The expression clearly points out the source of variation. To stabilize the variation, a simple bandwidth selection procedure is proposed. It is shown that the stabilized bandwidth selector gives a strongly consistent estimate of the optimal bandwidth. Under commonly used smoothness conditions, the stabilized bandwidth estimate has a faster convergence rate than the convergence rate of the cross-validation estimate. For sufficiently smooth density functions, it is shown that the stabilized bandwidth estimate is asymptotically normal with a relative convergence rate n-1/2 instead of the rate n-1/10 of the cross-validation estimate. A plug-in estimate and an adjusted plug-in estimate are also proposed, and their asymptotic distributions are obtained. It is noted that the plug-in estimate is asymptotically efficient. The adjusted plug-in bandwidth estimate and the stabilized bandwidth estimate are shown to be asymptotically equivalent. The simulation results verify that the proposed procedures perform much better than the cross-validation for finite samples.},
  File                     = {JSTOR Full Text PDF:files/390/Chiu - 1991 - Bandwidth Selection for Kernel Density Estimation.pdf:application/pdf},
  ISSN                     = {0090-5364},
  Owner                    = {yslin},
  Timestamp                = {2017.06.05},
  Url                      = {http://www.jstor.org/stable/2241909},
  Urldate                  = {2017-06-05}
}

@Article{Dawson1988,
  Title                    = {Fitting the ex-{Gaussian} equation to reaction time distributions},
  Author                   = {Dawson, Michael R. W.},
  Journal                  = {Behavior Research Methods, Instruments, \& Computers},
  Year                     = {1988},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {54--57},
  Volume                   = {20},

  Abstract                 = {Two programs that can be used to determine the probability distributions of reaction times are detailed. The first program takes rank-ordered reaction times as input and outputs a file of quantized data. The second program uses a simplex procedure to estimate the parameters of the ex-Gaussian equation that provides the best description of the quantized data. The advantages of this type of data analysis are also discussed.},
  Doi                      = {10.3758/BF03202603},
  File                     = {Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/9UD9V5AM/Dawson - 1988 - Fitting the ex-Gaussian equation to reaction time .pdf:application/pdf;Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/J6G2HBVQ/BF03202603.html:text/html},
  ISSN                     = {0743-3808, 1532-5970},
  Language                 = {en},
  Urldate                  = {2017-03-09}
}

@Article{Gelfand1990,
  Title                    = {Illustration of {Bayesian} {Inference} in {Normal} {Data} {Models} {Using} {Gibbs} {Sampling}},
  Author                   = {Gelfand, Alan E.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1990},

  Month                    = dec,
  Number                   = {412},
  Pages                    = {972--985},
  Volume                   = {85},

  Abstract                 = {The use of the Gibbs sampler as a method for calculating Bayesian marginal posterior and predictive densities is reviewed and illustrated with a range of normal data models, including variance components, unordered and ordered means, hierarchical growth curves, and missing data in a crossover trial. In all cases the approach is straightforward to specify distributionally and to implement computationally, with output readily adapted for required inference summaries.},
  Doi                      = {10.1080/01621459.1990.10474968},
  File                     = {Snapshot:files/381/01621459.1990.html:text/html},
  ISSN                     = {0162-1459},
  Owner                    = {yslin},
  Timestamp                = {2017.05.31},
  Url                      = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1990.10474968},
  Urldate                  = {2017-05-31}
}

@Book{gelman_bayesian_2014,
  Title                    = {Bayesian data analysis},
  Author                   = {Gelman, Andrew},
  Publisher                = {CRC Press},
  Year                     = {2014},

  Address                  = {Boca Raton},
  Note                     = {OCLC: 864304245},

  ISBN                     = {978-1-4398-4095-5 978-1-4398-4096-2},
  Language                 = {English}
}

@Article{Goldenshluger2011,
  Title                    = {Bandwidth selection in kernel in kernel density estimation: Oracle inequalities and adaptive minimax optimality},
  Author                   = {Goldenshluger, Alexander and Lepski, Oleg},
  Journal                  = {The Annals of Statistics},
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {1608--1632},
  Volume                   = {39},

  Abstract                 = {We address the problem of density estimation with 핃 s -loss by selection of kernel estimators. We develop a selection procedure and derive corresponding 핃 s -risk oracle inequalities. It is shown that the proposed selection rule leads to the estimator being minimax adaptive over a scale of the anisotropic Nikol'skii classes. The main technical tools used in our derivations are uniform bounds on the 핃 s -norms of empirical processes developed recently by Goldenshluger and Lepski [Ann. Probab. (2011), to appear].},
  File                     = {JSTOR Full Text PDF:files/392/Goldenshluger and Lepski - 2011 - BANDWIDTH SELECTION IN KERNEL DENSITY ESTIMATION .pdf:application/pdf},
  ISSN                     = {0090-5364},
  Owner                    = {yslin},
  Shorttitle               = {{BANDWIDTH} {SELECTION} {IN} {KERNEL} {DENSITY} {ESTIMATION}},
  Timestamp                = {2017.06.05},
  Url                      = {http://www.jstor.org/stable/23033609},
  Urldate                  = {2017-06-05}
}

@Article{Gutenkunst2007,
  Title                    = {Universally {Sloppy} {Parameter} {Sensitivities} in {Systems} {Biology} {Models}},
  Author                   = {Gutenkunst, Ryan N. and Waterfall, Joshua J. and Casey, Fergal P. and Brown, Kevin S. and Myers, Christopher R. and Sethna, James P.},
  Journal                  = {PLOS Computational Biology},
  Year                     = {2007},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {e189},
  Volume                   = {3},

  Abstract                 = {Author SummaryDynamic systems biology models typically involve many kinetic parameters, the quantitative determination of which has been a serious obstacle to using these models. Previous work showed for a particular model that useful predictions could be extracted from a fit long before the experimental data constrained the parameters, even to within orders of magnitude. This was attributed to a “sloppy” pattern in the model's parameter sensitivities; the sensitivity eigenvalues were roughly evenly spaced over many decades. Consequently, the model behavior depended effectively on only a few “stiff” parameter combinations. Here we study the converse problem, showing that direct parameter measurements are very inefficient at constraining the model's behavior. To yield effective predictions, such measurements must be very precise and complete; even a single imprecise parameter often destroys predictivity. We also show here that the characteristic sloppy eigenvalue pattern is reproduced in 16 other diverse models from the systems biology literature. The apparent universality of sloppiness suggests that predictions from most models will be very fragile to single uncertain parameters and that collective parameters fits can often yield tight predictions with loose parameters. Together these results argue that focusing on parameter values may be a very inefficient route to useful models.},
  Doi                      = {10.1371/journal.pcbi.0030189},
  File                     = {Full Text PDF:files/352/Gutenkunst et al. - 2007 - Universally Sloppy Parameter Sensitivities in Syst.pdf:application/pdf;Snapshot:files/353/article.html:text/html},
  ISSN                     = {1553-7358},
  Keywords                 = {Circadian rhythms, Confidence intervals, Eigenvalues, Eigenvectors, Ellipsoids, Experimental design, Metabolic networks, Systems biology},
  Owner                    = {yslin},
  Timestamp                = {2017.05.17},
  Urldate                  = {2017-05-17}
}

@Article{Harris2007,
  Title                    = {Optimizing parallel reduction in CUDA},
  Author                   = {Harris, Mark},
  Journal                  = {http://docs.nvidia.com/cuda/samples/6\_Advanced/reduction/doc/reduction.pdf},
  Year                     = {2007},

  Owner                    = {yslin},
  Timestamp                = {2017.05.16}
}

@Article{Heathcote2004,
  Title                    = {Fitting Wald and ex-Wald distributions to response time data: An example using functions for the S-PLUS package},
  Author                   = {Heathcote, Andrew},
  Journal                  = {Behavior Research Methods, Instruments, {\&} Computers},
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {678--694},
  Volume                   = {36},

  Abstract                 = {Schwarz (2001, 2002) proposed the ex-Wald distribution, obtained from the convolution of Wald and exponential random variables, as a model of simple and go/no-go response time. This article provides functions for the S-PLUS package that produce maximum likelihood estimates of the parameters for the ex-Wald, as well as for the shifted Wald and ex-Gaussian, distributions. In a Monte Carlo study, the efficiency and bias of parameter estimates were examined. Results indicated that samples of at least 400 are necessary to obtain adequate estimates of the ex-Wald and that, for some parameter ranges, much larger samples may be required. For shifted Wald estimation, smaller samples of around 100 were adequate, at least when fits identified by the software as having ill-conditioned maximums were excluded. The use of all functions is illustrated using data from Schwarz (2001). The S-PLUS functions and Schwarz's data may be downloaded from the Psychonomic Society's Web archive, www. psychonomic.org/archive/.},
  Doi                      = {10.3758/BF03206550},
  ISSN                     = {1532-5970},
  Owner                    = {yslin},
  Timestamp                = {2017.06.06}
}

@Article{Herculano-Houzel2009,
  Title                    = {The {Human} {Brain} in {Numbers}: {A} {Linearly} {Scaled}-up {Primate} {Brain}},
  Author                   = {Herculano-Houzel, Suzana},
  Journal                  = {Front Hum Neurosci},
  Year                     = {2009},

  Month                    = nov,
  Volume                   = {3},

  Abstract                 = {The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80\% of brain mass, and purportedly containing 100 billion neurons and 10× more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19\% of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain.},
  Doi                      = {10.3389/neuro.09.031.2009},
  File                     = {PubMed Central Full Text PDF:files/425/Herculano-Houzel - 2009 - The Human Brain in Numbers A Linearly Scaled-up P.pdf:application/pdf},
  ISSN                     = {1662-5161},
  Owner                    = {yslin},
  Pmcid                    = {PMC2776484},
  Pmid                     = {19915731},
  Shorttitle               = {The {Human} {Brain} in {Numbers}},
  Timestamp                = {2017.09.05},
  Urldate                  = {2017-09-05}
}

@InProceedings{Hermann2010,
  Title                    = {Multi-{GPU} and {Multi}-{CPU} parallelization for interactive physics simulations},
  Author                   = {Hermann, Everton and Raffin, Bruno and Faure, François and Gautier, Thierry and Allard, Jérémie},
  Booktitle                = {Euro-{Par} 2010 - {Parallel} {Processing}},
  Year                     = {2010},
  Month                    = aug,
  Pages                    = {235--246},
  Publisher                = {Springer, Berlin, Heidelberg},
  Series                   = {Lecture {Notes} in {Computer} {Science}},

  __markedentry            = {[yslin:]},
  Abstract                 = {Today, it is possible to associate multiple CPUs and multiple GPUs in a single shared memory architecture. Using these resources efficiently in a seamless way is a challenging issue. In this paper, we propose a parallelization scheme for dynamically balancing work load between multiple CPUs and GPUs. Most tasks have a CPU and GPU implementation, so they can be executed on any processing unit. We rely on a two level scheduling associating a traditional task graph partitioning and a work stealing guided by processor affinity and heterogeneity. These criteria are intended to limit inefficient task migrations between GPUs, the cost of memory transfers being high, and to favor mapping small tasks on CPUs and large ones on GPUs to take advantage of heterogeneity. This scheme has been implemented to support the SOFA physics simulation engine. Experiments show that we can reach speedups of 22 with 4 GPUs and 29 with 4 CPU cores and 4 GPUs. CPUs unload GPUs from small tasks making these GPUs more efficient, leading to a “cooperative speedup” greater than the sum of the speedups separatly obtained on 4 GPUs and 4 CPUs.},
  Doi                      = {10.1007/978-3-642-15291-7_23},
  File                     = {Full Text PDF:files/436/Hermann et al. - 2010 - Multi-GPU and Multi-CPU Parallelization for Intera.pdf:application/pdf;Snapshot:files/437/978-3-642-15291-7_23.html:text/html},
  ISBN                     = {978-3-642-15290-0 978-3-642-15291-7},
  Language                 = {en},
  Owner                    = {yslin},
  Timestamp                = {2017.09.08},
  Urldate                  = {2017-09-08}
}

@Article{hoffman2014_nut,
  Title                    = {The no-u-turn sampler: {Adaptively} setting path lengths in {Hamiltonian} {Monte} {Carlo}},
  Author                   = {Hoffman, Matthew D. and Gelman, Andrew},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2014},
  Number                   = {1},
  Volume                   = {15},

  Abstract                 = {Hierarchical Bayesian models are a mainstay of the machine learning and statistics communities. Exact posterior inference in such models is rarely tractable, so researchers and practitioners must usually resort to approximate inference methods. Perhaps the most popular class of approximate posterior inference algorithms, Markov Chain Monte Carlo (MCMC) methods offer schemes for},
  File                     = {Citeseer - Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/ZBVEJ72Z/Hoffman and Gelman - 2011 - The no-u-turn sampler Adaptively setting path len.pdf:application/pdf;Citeseer - Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/RSJW6ZFP/summary.html:text/html},
  Shorttitle               = {The no-u-turn sampler}
}

@Article{Hohle1965,
  Title                    = {Inferred components of reaction times as functions of foreperiod duration},
  Author                   = {Hohle, Raymond H.},
  Journal                  = {Journal of Experimental Psychology},
  Year                     = {1965},
  Number                   = {4},
  Pages                    = {382--386},
  Volume                   = {69},

  Abstract                 = {A distribution function representing simple reaction-time distributions was derived, assuming {RT} is the sum of 2 component variables with exponential and normal distributions. 4 Ss each gave 100 {RTs} to an auditory stimulus following each of 4 foreperiods, under each of 2 conditions: (a) foreperiod constant within sessions but varied over sessions, and (b) foreperiods appearing in a random sequence. The derived distribution function provided completely satisfactory representations of all 32 {RT} distributions, and the relations of the fitted parameters of this function to foreperiod suggest that the variation of {RT} as a function of foreperiod is due to variation in the normally distributed component.},
  Copyright                = {(c) 2012 {APA}, all rights reserved},
  Doi                      = {10.1037/h0021740},
  ISSN                     = {0022-1015(Print)},
  Keywords                 = {*Reaction Time, Stimulus Duration},
  Owner                    = {yslin},
  Timestamp                = {2017.05.10}
}

@Article{Holmes2015,
  Title                    = {A practical guide to the probability density approximation ({PDA}) with improved implementation and error characterization},
  Author                   = {Holmes, William R.},
  Journal                  = {Journal of Mathematical Psychology},
  Year                     = {2015},

  Month                    = oct,
  Pages                    = {13--24},
  Volume                   = {68--69},

  Abstract                 = {A critical task in modeling is to determine how well the theoretical assumptions encoded in a model account for observations. Bayesian methods are an ideal framework for doing just this. Existing approximate Bayesian computation (ABC) methods however rely on often insufficient “summary statistics”. Here, I present and analyze a highly efficient extension of the recently proposed (Turner and Sederberg 2014) Probability Density Approximation (PDA) method, which circumvents this insufficiency. This method combines Markov Chain Monte Carlo simulation with tools from non-parametric statistics to improve upon existing ABC methods. The primary contributions of this article are: (1) A more efficient implementation of this method that substantially improves computational performance is described. (2) Theoretical results describing the influence of methodological approximation errors on posterior estimation are discussed. In particular, while this method is highly accurate, even small errors have a strong influence on model comparisons when using standard statistical approaches (such as deviance information criterion). (3) An augmentation of the standard PDA procedure, termed “resampled PDA”, that reduces the negative influence of approximation errors on performance and accuracy, is presented. (4) A number of examples of varying complexity are presented along with supplementary code for their implementation.},
  Doi                      = {10.1016/j.jmp.2015.08.006},
  File                     = {ScienceDirect Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/K6GJHC6Z/Holmes - 2015 - A practical guide to the Probability Density Appro.pdf:application/pdf;ScienceDirect Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/2HKE7SKE/S0022249615000541.html:text/html},
  Keywords                 = {Approximate likelihood, Kernel density estimate, Linear ballistic accumulator model, Markov chain Monte Carlo, Non-parametric approximate Bayesian computation},
  Urldate                  = {2017-01-09}
}

@Article{Holmes2016,
  Title                    = {A new framework for modeling decisions about changing information: The Piecewise Linear Ballistic Accumulator model},
  Author                   = {William R. Holmes and Jennifer S. Trueblood and Andrew Heathcote},
  Journal                  = {Cognitive Psychology },
  Year                     = {2016},
  Pages                    = {1 - 29},
  Volume                   = {85},

  Abstract                 = {Abstract In the real world, decision making processes must be able to integrate non-stationary information that changes systematically while the decision is in progress. Although theories of decision making have traditionally been applied to paradigms with stationary information, non-stationary stimuli are now of increasing theoretical interest. We use a random-dot motion paradigm along with cognitive modeling to investigate how the decision process is updated when a stimulus changes. Participants viewed a cloud of moving dots, where the motion switched directions midway through some trials, and were asked to determine the direction of motion. Behavioral results revealed a strong delay effect: after presentation of the initial motion direction there is a substantial time delay before the changed motion information is integrated into the decision process. To further investigate the underlying changes in the decision process, we developed a Piecewise Linear Ballistic Accumulator model (PLBA). The \{PLBA\} is efficient to simulate, enabling it to be fit to participant choice and response-time distribution data in a hierarchal modeling framework using a non-parametric approximate Bayesian algorithm. Consistent with behavioral results, \{PLBA\} fits confirmed the presence of a long delay between presentation and integration of new stimulus information, but did not support increased response caution in reaction to the change. We also found the decision process was not veridical, as symmetric stimulus change had an asymmetric effect on the rate of evidence accumulation. Thus, the perceptual decision process was slow to react to, and underestimated, new contrary motion information. },
  Doi                      = {10.1016/j.cogpsych.2015.11.002},
  ISSN                     = {0010-0285},
  Keywords                 = {Evidence accumulation models},
  Owner                    = {yslin},
  Timestamp                = {2017.05.10}
}

@Article{Hu2010,
  Title                    = {Distributed Evolutionary Monte Carlo for Bayesian Computing},
  Author                   = {Hu, Bo and Tsui, Kam-Wah},
  Journal                  = {Comput. Stat. Data Anal.},
  Year                     = {2010},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {688--697},
  Volume                   = {54},

  Acmid                    = {1660744},
  Address                  = {Amsterdam, The Netherlands, The Netherlands},
  Doi                      = {10.1016/j.csda.2008.10.025},
  ISSN                     = {0167-9473},
  Issue_date               = {March, 2010},
  Numpages                 = {10},
  Owner                    = {yslin},
  Publisher                = {Elsevier Science Publishers B. V.},
  Timestamp                = {2017.06.08},
  Url                      = {sda.2008.10.025}
}

@Article{Kiani2008,
  Title                    = {Bounded Integration in Parietal Cortex Underlies Decisions Even When Viewing Duration Is Dictated by the Environment},
  Author                   = {Kiani, Roozbeh and Hanks, Timothy D. and Shadlen, Michael N.},
  Journal                  = {Journal of Neuroscience},
  Year                     = {2008},
  Number                   = {12},
  Pages                    = {3017--3029},
  Volume                   = {28},

  Abstract                 = {Decisions about sensory stimuli are often based on an accumulation of evidence in time. When subjects control stimulus duration, the decision terminates when the accumulated evidence reaches a criterion level. Under many natural circumstances and in many laboratory settings, the environment, rather than the subject, controls the stimulus duration. In these settings, it is generally assumed that subjects commit to a choice at the end of the stimulus stream. Indeed, failure to benefit from the full stream of information is interpreted as a sign of imperfect accumulation or memory leak. Contrary to these assumptions, we show that monkeys performing a direction discrimination task commit to a choice when the accumulated evidence reaches a threshold level (or bound), sometimes long before the end of stimulus. This bounded accumulation of evidence is reflected in the activity of neurons in the lateral intraparietal cortex. Thus, the readout of visual cortex embraces a termination rule to limit processing even when potentially useful information is available.},
  Doi                      = {10.1523/JNEUROSCI.4761-07.2008},
  Eprint                   = {http://www.jneurosci.org/content/28/12/3017.full.pdf},
  ISSN                     = {0270-6474},
  Owner                    = {yslin},
  Publisher                = {Society for Neuroscience},
  Timestamp                = {2017.05.24}
}

@Book{Kirk2017,
  Title                    = {Programming massively parallel processors: a hands-on approach},
  Author                   = {Kirk, David and Hwu, Wen-mei},
  Year                     = {2017},
  Note                     = {OCLC: 971352260},

  ISBN                     = {978-0-12-811986-0},
  Language                 = {English},
  Owner                    = {yslin},
  Shorttitle               = {Programming massively parallel processors},
  Timestamp                = {2017.09.07}
}

@Article{Kristan2011,
  Title                    = {Multivariate online kernel density estimation with Gaussian kernels },
  Author                   = {Matej Kristan and Aleš Leonardis and Danijel Skočaj},
  Journal                  = {Pattern Recognition },
  Year                     = {2011},
  Note                     = {Semi-Supervised Learning for Visual Content Analysis and Understanding },
  Number                   = {10–11},
  Pages                    = {2630 - 2642},
  Volume                   = {44},

  Abstract                 = {We propose a novel approach to online estimation of probability density functions, which is based on kernel density estimation (KDE). The method maintains and updates a non-parametric model of the observed data, from which the \{KDE\} can be calculated. We propose an online bandwidth estimation approach and a compression/revitalization scheme which maintains the KDE's complexity low. We compare the proposed online \{KDE\} to the state-of-the-art approaches on examples of estimating stationary and non-stationary distributions, and on examples of classification. The results show that the online \{KDE\} outperforms or achieves a comparable performance to the state-of-the-art and produces models with a significantly lower complexity while allowing online adaptation. },
  Doi                      = {https://doi.org/10.1016/j.patcog.2011.03.019},
  ISSN                     = {0031-3203},
  Keywords                 = {Online models},
  Owner                    = {yslin},
  Timestamp                = {2017.05.08},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0031320311001233}
}

@Article{Lerche2016,
  Title                    = {Model complexity in diffusion modeling: Benefits of making the model more parsimonious},
  Author                   = {Lerche, Veronika and Voss, Andreas},
  Journal                  = {Frontiers in Psychology},
  Year                     = {2016},

  Month                    = sep,
  Volume                   = {7},

  Abstract                 = {The diffusion model (Ratcliff, ) takes into account the reaction time distributions of both correct and erroneous responses from binary decision tasks. This high degree of information usage allows the estimation of different parameters mapping cognitive components such as speed of information accumulation or decision bias. For three of the four main parameters (drift rate, starting point, and non-decision time) trial-to-trial variability is allowed. We investigated the influence of these variability parameters both drawing on simulation studies and on data from an empirical test-retest study using different optimization criteria and different trial numbers. Our results suggest that less complex models (fixing intertrial variabilities of the drift rate and the starting point at zero) can improve the estimation of the psychologically most interesting parameters (drift rate, threshold separation, starting point, and non-decision time).},
  Doi                      = {10.3389/fpsyg.2016.01324},
  File                     = {PubMed Central Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/XXBUU6ZI/Lerche and Voss - 2016 - Model Complexity in Diffusion Modeling Benefits o.pdf:application/pdf},
  ISSN                     = {1664-1078},
  Owner                    = {yslin},
  Pmcid                    = {PMC5020081},
  Pmid                     = {27679585},
  Shorttitle               = {Model {Complexity} in {Diffusion} {Modeling}},
  Timestamp                = {2017.05.29},
  Urldate                  = {2017-01-09}
}

@Article{Matzke2009,
  Title                    = {Psychological interpretation of the ex-Gaussian and shifted Wald parameters: A diffusion model analysis},
  Author                   = {Matzke, Dora and Wagenmakers, Eric-Jan},
  Journal                  = {Psychonomic Bulletin \& Review},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {5},
  Pages                    = {798--817},
  Volume                   = {16},

  Abstract                 = {A growing number of researchers use descriptive distributions such as the ex-Gaussian and the shifted Wald to summarize response time data for speeded two-choice tasks. Some of these researchers also assume that the parameters of these distributions uniquely correspond to specific cognitive processes. We studied the validity of this cognitive interpretation by relating the parameters of the ex-Gaussian and shifted Wald distributions to those of the Ratcliff diffusion model, a successful model whose parameters have well-established cognitive interpretations. In a simulation study, we fitted the ex-Gaussian and shifted Wald distributions to data generated from the diffusion model by systematically varying its parameters across a wide range of plausible values. In an empirical study, the two descriptive distributions were fitted to published data that featured manipulations of task difficulty, response caution, and a priori bias. The results clearly demonstrate that the ex-Gaussian and shifted Wald parameters do not correspond uniquely to parameters of the diffusion model. We conclude that researchers should resist the temptation to interpret changes in the ex-Gaussian and shifted Wald parameters in terms of cognitive processes. Supporting materials may be downloaded from http://pbr.psychonomic-journals .org/content/supplemental.},
  Doi                      = {10.3758/PBR.16.5.798},
  File                     = {Full Text PDF:C\:\\Users\\user\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\8lsp7iop.default\\zotero\\storage\\CPTH5Q9X\\Matzke and Wagenmakers - 2009 - Psychological interpretation of the ex-Gaussian an.pdf:application/pdf;Snapshot:C\:\\Users\\user\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\8lsp7iop.default\\zotero\\storage\\9XR2FX2K\\PBR.16.5.html:text/html},
  ISSN                     = {1069-9384, 1531-5320},
  Keywords                 = {Cognitive Psychology},
  Language                 = {en},
  Owner                    = {yslin},
  Shorttitle               = {Psychological interpretation of the ex-Gaussian and shifted Wald parameters},
  Timestamp                = {2017.05.10},
  Urldate                  = {2014-06-04}
}

@Article{McClelland1979,
  Title                    = {On the time relations of mental processes: {An} examination of systems of processes in cascade},
  Author                   = {McClelland, James L.},
  Journal                  = {Psychological Review},
  Year                     = {1979},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {287--330},
  Volume                   = {86},

  Abstract                 = {Examines the possibility that the components of an information-processing system all operate continuously, passing information from one to the next as it becomes available. A model called the "cascade model" is presented and shown to be compatible with the general form of the relation between time and accuracy in speed-accuracy trade-off experiments. In the model, experimental manipulations may have either or both of 2 effects on a processing level: They may alter the rate of response or the asymptotic quality of the output. The effects of such manipulations on the output of a system of processes is described. The model is then used to reexamine the subtraction and additive factors methods for analyzing the composition of systems of processes. Results include the finding that factors that affect the rates of 2 different processes would be expected to have additive effects on reaction times under the cascade model, whereas 2 factors that both affect the rate of the same process would tend to interact, just as in the case in which the manipulations affect the durations of discrete stages. Factors that affect asymptotic output, however, tend to interact whether they affect the same or different processes. A new method is presented for analyzing processes in cascade, which extends the additive factors method to an analysis of the parameters of the function relating response time and accuracy. (70 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved), (C) 1979 by the American Psychological Association},
  ISSN                     = {0033-295X},
  Keywords                 = {cascade model of information processing},
  Language                 = {English.},
  Owner                    = {yslin},
  Shorttitle               = {On the time relations of mental processes},
  Timestamp                = {2017.05.11}
}

@Book{McElreath2016,
  Title                    = {Statistical rethinking: a {Bayesian} course with examples in {R} and {Stan}},
  Author                   = {McElreath, Richard},
  Year                     = {2016},
  Note                     = {OCLC: 956953600},

  __markedentry            = {[yslin:6]},
  ISBN                     = {978-1-4822-5346-7},
  Language                 = {English},
  Owner                    = {yslin},
  Shorttitle               = {Statistical rethinking},
  Timestamp                = {2017.09.20},
  Urldate                  = {2017-09-20}
}

@Article{Parzen1962,
  Title                    = {On Estimation of a Probability Density Function and Mode},
  Author                   = {Parzen, Emanuel},
  Journal                  = {The Annals of Mathematical Statistics},
  Year                     = {1962},

  Month                    = {09},
  Number                   = {3},
  Pages                    = {1065--1076},
  Volume                   = {33},

  Doi                      = {10.1214/aoms/1177704472},
  Fjournal                 = {The Annals of Mathematical Statistics},
  Owner                    = {yslin},
  Publisher                = {The Institute of Mathematical Statistics},
  Timestamp                = {2017.05.09}
}

@Book{Price2014,
  Title                    = {Differential evolution.},
  Author                   = {Price, Kenneth},
  Publisher                = {Springer},
  Year                     = {2014},

  Address                  = {Place of publication not identified},
  Note                     = {OCLC: 902764923},
  ISBN                     = {978-3-642-42416-8},
  Language                 = {English},
  Owner                    = {yslin},
  Timestamp                = {2017.09.07}
}

@Article{Ratcliff1978,
  Title                    = {A theory of memory retrieval},
  Author                   = {Ratcliff, Roger},
  Journal                  = {Psychological Review},
  Year                     = {1978},
  Number                   = {2},
  Pages                    = {59--108},
  Volume                   = {85},

  Abstract                 = {Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe–memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe–memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study–test, and continuous) and to speed–accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.},
  Copyright                = {(c) 2016 APA, all rights reserved},
  Doi                      = {10.1037/0033-295X.85.2.59},
  ISSN                     = {1939-1471 0033-295X},
  Keywords                 = {*Memory, Theories},
  Language                 = {English},
  Owner                    = {yslin},
  Timestamp                = {2017.05.19}
}

@Article{RatcliffChilders2015,
  Title                    = {Individual differences and fitting methods for the two-choice diffusion model of decision making.},
  Author                   = {Ratcliff, Roger and Childers, Russ},
  Journal                  = {Decision},
  Year                     = {2015},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {237--279},
  Volume                   = {2},

  Abstract                 = {Methods of fitting the diffusion model were examined with a focus on what the model can tell us about individual differences. Diffusion model parameters were obtained from the fits to data from 2 experiments and consistency of parameter values, individual differences, and practice effects were examined using different numbers of observations from each subject. Two issues were examined—first, what sizes of differences between groups can be obtained to distinguish between groups, and second, what sizes of differences would be needed to find individual subjects that had a deficit relative to a control group. The parameter values from the experiments provided ranges that were used in a simulation study to examine recovery of individual differences. This study used several diffusion model fitting programs, fitting methods, and published packages. In a second simulation study, 64 sets of simulated data from each of 48 sets of parameter values (spanning the range of typical values obtained from fits to data) were fit with the different methods, and biases and standard deviations in recovered model parameters were compared across methods. Finally, in a third simulation study, a comparison between a standard chi-square method and a hierarchical Bayesian method was performed. The results from these studies can be used as a starting point for selecting fitting methods, and as a basis for understanding the strengths and weaknesses of using diffusion model analyses to examine individual differences in clinical, neuropsychological, and educational testing. (PsycINFO Database Record (c) 2015 APA, all rights reserved)(journal abstract)},
  Copyright                = {© American Psychological Association 2015},
  Doi                      = {http://dx.doi.org/10.1037/dec0000030},
  File                     = {Full Text PDF:files/130/Ratcliff and Childers - 2015 - Individual differences and fitting methods for the.pdf:application/pdf},
  ISSN                     = {2325-9965},
  Keywords                 = {Choice Behavior (major), Decision Making (major), Educational Measurement, Individual Differences (major), Models (major), Neuropsychological Assessment, Reaction Time (major)},
  Language                 = {English},
  Owner                    = {yslin},
  Timestamp                = {2017.08.07},
  Urldate                  = {2016-01-13}
}

@Article{Ratcliff2008,
  Title                    = {The {Diffusion} {Decision} {Model}: {Theory} and {Data} for {Two}-{Choice} {Decision} {Tasks}},
  Author                   = {Ratcliff, Roger and McKoon, Gail},
  Journal                  = {Neural Computation},
  Year                     = {2008},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {873--922},
  Volume                   = {20},

  Abstract                 = {The diffusion decision model allows detailed explanations of behavior in two-choice discrimination tasks. In this article, the model is reviewed to show how it translates behavioral data—accuracy, mean response times, and response time distributions—into components of cognitive processing. Three experiments are used to illustrate experimental manipulations of three components: stimulus difficulty affects the quality of information on which a decision is based; instructions emphasizing either speed or accuracy affect the criterial amounts of information that a subject requires before initiating a response; and the relative proportions of the two stimuli affect biases in drift rate and starting point. The experiments also illustrate the strong constraints that ensure the model is empirically testable and potentially falsifiable. The broad range of applications of the model is also reviewed, including research in the domains of aging and neurophysiology.},
  Doi                      = {10.1162/neco.2008.12-06-420},
  File                     = {IEEE Xplore Abstract Record:files/86/freeabs_all.html:text/html},
  ISSN                     = {0899-7667},
  Owner                    = {yslin},
  Shorttitle               = {The {Diffusion} {Decision} {Model}},
  Timestamp                = {2017.05.12}
}

@Article{Ratcliff2002,
  Title                    = {Estimating parameters of the diffusion model: {Approaches} to dealing with contaminant reaction times and parameter variability},
  Author                   = {Ratcliff, Roger and Tuerlinckx, Francis},
  Journal                  = {Psychonomic Bulletin \& Review},
  Year                     = {2002},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {438--481},
  Volume                   = {9},

  Abstract                 = {Three methods for fitting the diffusion model (Ratcliff, 1978) to experimental data are examined. Sets of simulated data were generated with known parameter values, and from fits of the model, we found that the maximum likelihood method was better than the chi-square and weighted least squares methods by criteria of bias in the parameters relative to the parameter values used to generate the data and standard deviations in the parameter estimates. The standard deviations in the parameter values can be used as measures of the variability in parameter estimates from fits to experimental data. We introduced contaminant reaction times and variability into the other components of processing besides the decision process and found that the maximum likelihood and chi-square methods failed, sometimes dramatically. But the weighted least squares method was robust to these two factors. We then present results from modifications of the maximum likelihood and chi-square methods, in which these factors are explicitly modeled, and show that the parameter values of the diffusion model are recovered well. We argue that explicit modeling is an important method for addressing contaminants and variability in nondecision processes and that it can be applied in any theoretical approach to modeling reaction time.},
  Doi                      = {10.3758/BF03196302},
  File                     = {Full Text PDF:files/49/Ratcliff and Tuerlinckx - 2002 - Estimating parameters of the diffusion model Appr.pdf:application/pdf;Snapshot:files/50/BF03196302.html:text/html},
  ISSN                     = {1069-9384, 1531-5320},
  Keywords                 = {Cognitive Psychology},
  Language                 = {en},
  Owner                    = {yslin},
  Shorttitle               = {Estimating parameters of the diffusion model},
  Timestamp                = {2017.05.29},
  Urldate                  = {2015-12-17}
}

@Article{Ravenzwaaij2009,
  Title                    = {How to use the diffusion model: {Parameter} recovery of three methods: {EZ}, fast-dm, and {DMAT}},
  Author                   = {van Ravenzwaaij, Don and Oberauer, Klaus},
  Journal                  = {Journal of Mathematical Psychology},
  Year                     = {2009},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {463--473},
  Volume                   = {53},

  Abstract                 = {Parameter recovery of three different implementations of the Ratcliff diffusion model was investigated: the EZ model (Wagenmakers, van der Maas, \&amp; Grasman, 2007), fast-dm (Voss \&amp; Voss, 2007), and DMAT (Vandekerckhove \&amp; Tuerlinckx, 2007). Their capacity to recover both the mean structure and individual differences in parameter values was explored. The three methods were applied to simulated data generated by the diffusion model, by the leaky, competing accumulator (LCA) model (Usher \&amp; McClelland, 2001) and by the linear ballistic accumulator (LBA) model (Brown \&amp; Heathcote, 2008). Results show that EZ and DMAT are better capable than fast-dm in recovering experimental effects on parameters. EZ was best in recovering individual differences in parameter values. When data were generated by the LCA model, the diffusion model estimates obtained with all three methods correlated well with corresponding LCA model parameters. No such one-on-one correspondence could be established between parameters of the LBA model and the diffusion model.},
  Doi                      = {10.1016/j.jmp.2009.09.004},
  File                     = {ScienceDirect Full Text PDF:files/191/van Ravenzwaaij and Oberauer - 2009 - How to use the diffusion model Parameter recovery.pdf:application/pdf;ScienceDirect Snapshot:files/190/S0022249609001187.html:text/html},
  ISSN                     = {0022-2496},
  Keywords                 = {Diffusion model, Parameter estimation, Reaction times},
  Owner                    = {yslin},
  Shorttitle               = {How to use the diffusion model},
  Timestamp                = {2017.05.29},
  Urldate                  = {2017-01-09}
}

@Article{Sanderson2016,
  Title                    = {Armadillo: a template-based {C}++ library for linear algebra},
  Author                   = {Sanderson, Conrad and Curtin, Ryan},
  Journal                  = {The Journal of Open Source Software},
  Year                     = {2016},

  Month                    = jun,
  Number                   = {2},
  Volume                   = {1},

  Doi                      = {10.21105/joss.00026},
  Shorttitle               = {Armadillo},
  Urldate                  = {2017-03-09}
}

@Article{Schwarz2002,
  Title                    = {On the convulution of inverse Gaussian and exponential random variables},
  Author                   = {Wolf Schwarz},
  Journal                  = {Communications in Statistics - Theory and Methods},
  Year                     = {2002},
  Number                   = {12},
  Pages                    = {2113-2121},
  Volume                   = {31},

  Doi                      = {10.1081/STA-120017215},
  Eprint                   = { 
 http://dx.doi.org/10.1081/STA-120017215
 
},
  Owner                    = {yslin},
  Timestamp                = {2017.06.06}
}

@Book{Silverman1986,
  Title                    = {Density estimation for statistics and data analysis},
  Author                   = {Silverman, Bernard W},
  Publisher                = {CRC press},
  Year                     = {1986},
  Volume                   = {26}
}

@Article{Sisson2010,
  Title                    = {Likelihood-free {Markov} chain {Monte} {Carlo}},
  Author                   = {Sisson, S. A. and Fan, Y.},
  Journal                  = {arXiv:1001.2058 [stat]},
  Year                     = {2010},

  Month                    = jan,
  Note                     = {arXiv: 1001.2058},

  Abstract                 = {To appear to MCMC handbook, S. P. Brooks, A. Gelman, G. Jones and X.-L. Meng (eds), Chapman \& Hall.},
  File                     = {arXiv\:1001.2058 PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/29ZWJNE5/Sisson and Fan - 2010 - Likelihood-free Markov chain Monte Carlo.pdf:application/pdf;arXiv.org Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/GQACJXXU/1001.html:text/html},
  Keywords                 = {Statistics - Methodology},
  Url                      = {http://arxiv.org/abs/1001.2058},
  Urldate                  = {2017-03-09}
}

@Article{Smith2016,
  Title                    = {Diffusion theory of decision making in continuous report},
  Author                   = {Smith, Philip L.},
  Journal                  = {Psychological Review},
  Year                     = {2016},
  Number                   = {4},
  Pages                    = {425--451},
  Volume                   = {123},

  Abstract                 = {I present a diffusion model for decision making in continuous report tasks, in which a continuous, circularly distributed, stimulus attribute in working memory is matched to a representation of the attribute in the stimulus display. Memory retrieval is modeled as a 2-dimensional diffusion process with vector-valued drift on a disk, whose bounding circle represents the decision criterion. The direction and magnitude of the drift vector describe the identity of the stimulus and the quality of its representation in memory, respectively. The point at which the diffusion exits the disk determines the reported value of the attribute and the time to exit the disk determines the decision time. Expressions for the joint distribution of decision times and report outcomes are obtained by means of the Girsanov change-of-measure theorem, which allows the properties of the nonzero-drift diffusion process to be characterized as a function of a Euclidian-distance Bessel process. Predicted report precision is equal to the product of the decision criterion and the drift magnitude and follows a von Mises distribution, in agreement with the treatment of precision in the working memory literature. Trial-to-trial variability in criterion and drift rate leads, respectively, to direct and inverse relationships between report accuracy and decision times, in agreement with, and generalizing, the standard diffusion model of 2-choice decisions. The 2-dimensional model provides a process account of working memory precision and its relationship with the diffusion model, and a new way to investigate the properties of working memory, via the distributions of decision times.},
  Copyright                = {(c) 2016 APA, all rights reserved},
  Doi                      = {10.1037/rev0000023},
  ISSN                     = {1939-1471 0033-295X},
  Keywords                 = {*Decision Making, *Models, *Short Term Memory, Theories},
  Language                 = {English},
  Owner                    = {yslin},
  Timestamp                = {2017.06.11}
}

@Article{Spiegelhalter2002,
  Title                    = {Bayesian measures of model complexity and fit},
  Author                   = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Van Der Linde, Angelika},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2002},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {583--639},
  Volume                   = {64},

  Abstract                 = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the ‘hat’ matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  Doi                      = {10.1111/1467-9868.00353},
  File                     = {Full Text PDF:files/410/Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf:application/pdf;Snapshot:files/409/abstract\;jsessionid=A7C3845A182E4217B94070A288DE2B5F.html:text/html},
  ISSN                     = {1467-9868},
  Keywords                 = {Bayesian model comparison, Decision theory, Deviance information criterion, Effective number of parameters, Hierarchical models, Information theory, Leverage, Markov chain Monte Carlo methods, Model dimension},
  Language                 = {en},
  Owner                    = {yslin},
  Timestamp                = {2017.08.08},
  Urldate                  = {2017-08-07}
}

@Article{Tsetsos2012,
  Title                    = {Using {Time}-{Varying} {Evidence} to {Test} {Models} of {Decision} {Dynamics}: {Bounded} {Diffusion} vs. the {Leaky} {Competing} {Accumulator} {Model}},
  Author                   = {Tsetsos, Konstantinos and Gao, Juan and McClelland, James L. and Usher, Marius},
  Journal                  = {Frontiers in Neuroscience},
  Year                     = {2012},

  Month                    = jun,
  Volume                   = {6},

  Abstract                 = {When people make decisions, do they give equal weight to evidence arriving at different times? A recent study (Kiani et al., ) using brief motion pulses (superimposed on a random moving dot display) reported a primacy effect: pulses presented early in a motion observation period had a stronger impact than pulses presented later. This observation was interpreted as supporting the bounded diffusion (BD) model and ruling out models in which evidence accumulation is subject to leakage or decay of early-arriving information. We use motion pulses and other manipulations of the timing of the perceptual evidence in new experiments and simulations that support the leaky competing accumulator (LCA) model as an alternative to the BD model. While the LCA does include leakage, we show that it can exhibit primacy as a result of competition between alternatives (implemented via mutual inhibition), when the inhibition is strong relative to the leak. Our experiments replicate the primacy effect when participants must be prepared to respond quickly at the end of a motion observation period. With less time pressure, however, the primacy effect is much weaker. For 2 (out of 10) participants, a primacy bias observed in trials where the motion observation period is short becomes weaker or reverses (becoming a recency effect) as the observation period lengthens. Our simulation studies show that primacy is equally consistent with the LCA or with BD. The transition from primacy-to-recency can also be captured by the LCA but not by BD. Individual differences and relations between the LCA and other models are discussed.},
  Doi                      = {10.3389/fnins.2012.00079},
  File                     = {PubMed Central Full Text PDF:files/370/Tsetsos et al. - 2012 - Using Time-Varying Evidence to Test Models of Deci.pdf:application/pdf},
  ISSN                     = {1662-4548},
  Owner                    = {yslin},
  Pmcid                    = {PMC3372959},
  Pmid                     = {22701399},
  Shorttitle               = {Using {Time}-{Varying} {Evidence} to {Test} {Models} of {Decision} {Dynamics}},
  Timestamp                = {2017.05.24},
  Urldate                  = {2017-05-24}
}

@Article{Turner2013,
  Title                    = {Likelihood-free {Bayesian} analysis of memory models},
  Author                   = {Turner, Brandon M. and Dennis, Simon and Van Zandt, Trisha},
  Journal                  = {Psychological Review},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {667--678},
  Volume                   = {120},

  Abstract                 = {Many influential memory models are computational in the sense that their predictions are derived through simulation. This means that it is difficult or impossible to write down a probability distribution or likelihood that characterizes the random behavior of the data as a function of the model’s parameters. In turn, the lack of a likelihood means that these models cannot be directly fitted to data using traditional techniques. In particular, standard Bayesian analyses of such models are impossible. In this article, we examine how a new procedure called approximate Bayesian computation (ABC), a method for Bayesian analysis that circumvents the evaluation of the likelihood, can be used to fit computational models to memory data. In particular, we investigate the bind cue decide model of episodic memory (Dennis \& Humphreys, 2001) and the retrieving effectively from memory model (Shiffrin \& Steyvers, 1997). We fit hierarchical versions of each model to the data of Dennis, Lee, and Kinnell (2008) and Kinnell and Dennis (2012). The ABC analysis permits us to explore the relationships between the parameters in each model as well as evaluate their relative fits to data—analyses that were not previously possible.},
  Copyright                = {(c) 2016 APA, all rights reserved},
  Doi                      = {10.1037/a0032458},
  ISSN                     = {1939-1471 0033-295X},
  Keywords                 = {*Computational Modeling, *Memory, *Models, *Simulation, Statistical Probability},
  Language                 = {English},
  Owner                    = {yslin},
  Timestamp                = {2017.03.10}
}

@Article{Turner2014,
  Title                    = {A generalized, likelihood-free method for posterior estimation},
  Author                   = {Turner, Brandon M. and Sederberg, Per B.},
  Journal                  = {Psychonomic Bulletin \& Review},
  Year                     = {2014},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {227--250},
  Volume                   = {21},

  Abstract                 = {Recent advancements in Bayesian modeling have allowed for likelihood-free posterior estimation. Such estimation techniques are crucial to the understanding of simulation-based models, whose likelihood functions may be difficult or even impossible to derive. However, current approaches are limited by their dependence on sufficient statistics and/or tolerance thresholds. In this article, we provide a new approach that requires no summary statistics, error terms, or thresholds and is generalizable to all models in psychology that can be simulated. We use our algorithm to fit a variety of cognitive models with known likelihood functions to ensure the accuracy of our approach. We then apply our method to two real-world examples to illustrate the types of complex problems our method solves. In the first example, we fit an error-correcting criterion model of signal detection, whose criterion dynamically adjusts after every trial. We then fit two models of choice response time to experimental data: the linear ballistic accumulator model, which has a known likelihood, and the leaky competing accumulator model, whose likelihood is intractable. The estimated posterior distributions of the two models allow for direct parameter interpretation and model comparison by means of conventional Bayesian statistics—a feat that was not previously possible.},
  Doi                      = {10.3758/s13423-013-0530-0},
  File                     = {Full Text PDF:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/HWE5B3X8/Turner and Sederberg - 2014 - A generalized, likelihood-free method for posterio.pdf:application/pdf;Snapshot:/home/yslin/.mozilla/firefox/dgi432js.default/zotero/storage/2D4UKPPC/s13423-013-0530-0.html:text/html},
  ISSN                     = {1069-9384, 1531-5320},
  Language                 = {en},
  Urldate                  = {2017-03-09}
}

@Article{Turner2012,
  Title                    = {Approximate Bayesian computation with differential evolution },
  Author                   = {Brandon M. Turner and Per B. Sederberg},
  Journal                  = {Journal of Mathematical Psychology },
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {375 - 385},
  Volume                   = {56},

  Abstract                 = {Approximate Bayesian computation (ABC) is a simulation-based method for estimating the posterior distribution of the parameters of a model. The \{ABC\} approach is instrumental when a likelihood function for a model cannot be mathematically specified, or has a complicated form. Although difficulty in calculating a model’s likelihood is extremely common, current \{ABC\} methods suffer from two problems that have largely prevented their mainstream adoption: long computation time and an inability to scale beyond a few parameters. We introduce differential evolution as a computationally efficient genetic algorithm for proposal generation in our \{ABC\} sampler. We show how using this method allows our new \{ABC\} algorithm, called ABCDE, to obtain accurate posterior estimates in fewer iterations than kernel-based \{ABC\} algorithms and to scale to high-dimensional parameter spaces that have proven difficult for current \{ABC\} methods. },
  Doi                      = {https://doi.org/10.1016/j.jmp.2012.06.004},
  ISSN                     = {0022-2496},
  Keywords                 = {Approximate Bayesian computation},
  Owner                    = {yslin},
  Timestamp                = {2017.06.08}
}

@Article{Turner2013a,
  Title                    = {A method for efficiently sampling from distributions with correlated dimensions.},
  Author                   = {Turner, Brandon M and Sederberg, Per B and Brown, Scott D and Steyvers, Mark},
  Journal                  = {Psychological methods},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {368},
  Volume                   = {18},

  Owner                    = {yslin},
  Publisher                = {American Psychological Association},
  Timestamp                = {2017.09.06}
}

@Article{VanZandt2000,
  Title                    = {How to fit a response time distribution},
  Author                   = {Van Zandt, Trisha},
  Journal                  = {Psychonomic Bulletin {\&} Review},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {424--465},
  Volume                   = {7},

  Abstract                 = {Among the most valuable tools in behavioral science is statistically fitting mathematical models of cognition to data---response time distributions, in particular. However, techniques for fitting distributions vary widely, and little is known about the efficacy of different techniques. In this article, we assess several fitting techniques by simulating six widely cited models of response time and using the fitting procedures to recover model parameters. The techniques include the maximization of likelihood and least squares fits of the theoretical distributions to different empirical estimates of the simulated distributions. A running example is used to illustrate the different estimation and fitting procedures. The simulation studies reveal that empirical density estimates are biased even for very large sample sizes. Some fitting techniques yield more accurate and less variable parameter estimates than do others. Methods that involve least squares fits to density estimates generally yield very poor parameter estimates.},
  Doi                      = {10.3758/BF03214357},
  ISSN                     = {1531-5320},
  Owner                    = {yslin},
  Timestamp                = {2017.05.09}
}

@Article{Verdonck2016,
  Title                    = {Efficient simulation of diffusion-based choice RT models on CPU and GPU},
  Author                   = {Verdonck, Stijn
and Meers, Kristof
and Tuerlinckx, Francis},
  Journal                  = {Behavior Research Methods},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {13--27},
  Volume                   = {48},

  Abstract                 = {In this paper, we present software for the efficient simulation of a broad class of linear and nonlinear diffusion models for choice RT, using either CPU or graphical processing unit (GPU) technology. The software is readily accessible from the popular scripting languages MATLAB and R (both 64-bit). The speed obtained on a single high-end GPU is comparable to that of a small CPU cluster, bringing standard statistical inference of complex diffusion models to the desktop platform.},
  Doi                      = {10.3758/s13428-015-0569-0},
  ISSN                     = {1554-3528},
  Owner                    = {yslin},
  Timestamp                = {2017.05.15}
}

@Article{Wagenmakers2008,
  Title                    = {A diffusion model account of criterion shifts in the lexical decision task},
  Author                   = {Wagenmakers, Eric-Jan and Ratcliff, Roger and Gomez, Pablo and McKoon, Gail},
  Journal                  = {Journal of Memory and Language},
  Year                     = {2008},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {140--159},
  Volume                   = {58},

  Abstract                 = {Performance in the lexical decision task is highly dependent on decision criteria. These criteria can be influenced by speed versus accuracy instructions and word/nonword proportions. Experiment 1 showed that error responses speed up relative to correct responses under instructions to respond quickly. Experiment 2 showed that responses to less probable stimuli are slower and less accurate than responses to more probable stimuli. The data from both experiments support the diffusion model for lexical decision [Ratcliff, R., Gomez, P., \&amp; McKoon, G. (2004a). A diffusion model account of the lexical decision task. Psychological Review, 111, 159–182]. At the same time, the data provide evidence against the popular deadline model for lexical decision. The deadline model assumes that “nonword” responses are given only after the “word” response has timed out—consequently, the deadline model cannot account for the data from experimental conditions in which “nonword” responses are systematically faster than “word” responses.},
  Doi                      = {10.1016/j.jml.2007.04.006},
  File                     = {ScienceDirect Full Text PDF:files/59/Wagenmakers et al. - 2008 - A diffusion model account of criterion shifts in t.pdf:application/pdf;ScienceDirect Full Text PDF:files/222/Wagenmakers et al. - 2008 - A diffusion model account of criterion shifts in t.pdf:application/pdf;ScienceDirect Snapshot:files/58/S0749596X07000496.html:text/html;ScienceDirect Snapshot:files/221/S0749596X07000496.html:text/html},
  ISSN                     = {0749-596X},
  Keywords                 = {Deadline model, Diffusion model, DRC, Lexical decision, MROM, Response criteria},
  Owner                    = {yslin},
  Timestamp                = {2017.09.05},
  Urldate                  = {2015-12-17}
}

@Book{Wilt2013,
  Title                    = {The {CUDA} handbook a comprehensive guide to {GPU} programming},
  Author                   = {Wilt, Nicholas},
  Publisher                = {Addison-Wesley},
  Year                     = {2013},

  Address                  = {Upper Saddle River, NJ},
  Note                     = {OCLC: 929437891},

  ISBN                     = {978-0-321-80946-9 978-0-13-326151-6},
  Language                 = {English},
  Owner                    = {yslin},
  Timestamp                = {2017.09.07},
  Url                      = {http://0proquest.safaribooksonline.com/9780133261516},
  Urldate                  = {2017-09-07}
}

